{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf7c4e96-cd36-4673-bd16-e1af13757422",
   "metadata": {},
   "source": [
    "# Use gpt-3.5-turbo to combine the pix2text & easy_ocr outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ccd9b9-0986-47f8-8be8-45de7f94b8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === kernel system ===\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import shutil\n",
    "import traceback\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "# === packages ===\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from openai import OpenAI\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0126c17-6034-474a-b397-cc2aca8a4f1c",
   "metadata": {},
   "source": [
    "# Load api keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e67f5e1-dbfb-4e66-a5a7-54a8c8148240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_keys(path):\n",
    "    keys = {}\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            if \"=\" in line:\n",
    "                k, v = line.strip().split(\"=\", 1)\n",
    "                keys[k] = v\n",
    "    return keys\n",
    "\n",
    "keys = load_keys(\"/home/ne6131039/Desktop/Confidential_Key.txt\")\n",
    "openai_key = keys['OPENAI_KEY']\n",
    "huggingface_write_key = keys['HUGGINGFACE_WRITE_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9facb77d-f59b-4723-af58-c6c016a7184e",
   "metadata": {},
   "source": [
    "# Merge two type of text and label caption_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffa26da-5353-4900-b0a2-47c90af3193d",
   "metadata": {},
   "source": [
    "### System prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f20f25-9209-433a-8d8d-1e446ede7bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a scientific caption corrector and classifier.\n",
    "\n",
    "You are given two OCR outputs from a scientific figure caption:\n",
    "- One from Pix2Text (in LaTeX-style format)\n",
    "- One from EasyOCR (standard plain text)\n",
    "\n",
    "Your job is to:\n",
    "1. Reconstruct the most accurate and readable caption in proper English.\n",
    "2. Classify the content into one of the following categories:\n",
    "   [\"nanomaterials\", \"bioimaging\", \"crystallography\", \"device structures\", \"other\"]\n",
    "\n",
    "--- Output format ---\n",
    "caption_content: <your refined caption>\n",
    "caption_type: <category>\n",
    "\n",
    "Note: \n",
    "-Always use Unicode superscript formatting for units (e.g., m⁻², cm⁻³).\\\n",
    "-Preserve chemical formulas as they appear (e.g., CsPbBr₃, LaNi₀.₅Cu₀.₁O₃) without expanding them into full names.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d806e1-e497-46b8-8666-9524e8fa32db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=openai_key)  \n",
    "\n",
    "# Constants\n",
    "INPUT_CSV = \"../caption_analysis/ocr_output.csv\"\n",
    "OUTPUT_CSV = \"caption_analysis_output.csv\"\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "#Initialize index \n",
    "START_INDEX = 0  \n",
    "\n",
    "# Load input\n",
    "df = pd.read_csv(INPUT_CSV).fillna(\"\")\n",
    "total_rows = len(df)\n",
    "# total_rows = 40000\n",
    "\n",
    "# Check how many already processed\n",
    "if os.path.exists(OUTPUT_CSV):\n",
    "    output_df = pd.read_csv(OUTPUT_CSV)\n",
    "    processed_images = set(output_df[\"image_name\"])\n",
    "else:\n",
    "    output_df = pd.DataFrame(columns=[\"image_name\", \"caption_content\", \"caption_type\"])\n",
    "    processed_images = set()\n",
    "\n",
    "# Process in batches\n",
    "for idx in range(START_INDEX, total_rows):\n",
    "    row = df.iloc[idx]\n",
    "    image_name = row[\"image_name\"]\n",
    "\n",
    "    # Skip if already processed\n",
    "    if image_name in processed_images:\n",
    "        continue\n",
    "\n",
    "    print(f\" Processing {idx+1}/{total_rows}: {image_name}\")\n",
    "\n",
    "    combined_text = f\"\"\"\n",
    "[Pix2Text OCR]:\n",
    "{row['pix2text_ocr'].strip()}\n",
    "\n",
    "[EasyOCR OCR]:\n",
    "{row['easyocr_ocr'].strip()}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": combined_text}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            max_tokens=4000\n",
    "        )\n",
    "\n",
    "        reply = response.choices[0].message.content.strip()\n",
    "        lines = reply.splitlines()\n",
    "        \n",
    "        caption_content = \"\"\n",
    "        caption_type = \"\"\n",
    "        \n",
    "        for line in lines:\n",
    "            if line.startswith(\"caption_content:\"):\n",
    "                caption_content = line.replace(\"caption_content:\", \"\").strip()\n",
    "            elif line.startswith(\"caption_type:\"):\n",
    "                caption_type = line.replace(\"caption_type:\", \"\").strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        caption_content = f\"[ERROR] {e}\"\n",
    "        caption_type = \"unknown\"\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Save result\n",
    "    new_row = pd.DataFrame([{\n",
    "        \"image_name\": image_name,\n",
    "        \"caption_content\": caption_content,\n",
    "        \"caption_type\": caption_type\n",
    "    }])\n",
    "\n",
    "    new_row.to_csv(OUTPUT_CSV, mode=\"a\", header=not os.path.exists(OUTPUT_CSV), index=False)\n",
    "\n",
    "    # Optional delay to avoid rate limits\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print (\"Processing Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd78e4a-476a-4ca7-9964-d716d030ebb1",
   "metadata": {},
   "source": [
    "# Choose [nanomaterials,crystallography] images\n",
    "> remove bio data and balance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536be03c-838a-430c-9a29-f3f77472556f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read CSV files for analysis\n",
    "caption_df = pd.read_csv(\"caption_analysis_output.csv\")\n",
    "tem_df = pd.read_csv(\"../tem_images_description.csv\")\n",
    "\n",
    "# Filter desired caption types\n",
    "target_types = [\"nanomaterials\",\"crystallography\"]\n",
    "valid_parents = caption_df[caption_df[\"caption_type\"].isin(target_types)][\"image_name\"].unique()\n",
    "\n",
    "# Filter sub-images corresponding to selected parent images\n",
    "matched_df = tem_df[tem_df[\"parent_image\"].isin(valid_parents)]\n",
    "\n",
    "# Sample 10,000 sub-images for each TEM type (if available)\n",
    "balanced_subs = (\n",
    "    matched_df\n",
    "    .groupby(\"TEM_type\", group_keys=False)\n",
    "    .apply(lambda g: g.sample(n=10000, random_state=42) if len(g) >= 10000 else g)\n",
    ")\n",
    "\n",
    "print(\"number of every TEM_type ：\")\n",
    "print(balanced_subs[\"TEM_type\"].value_counts())\n",
    "\n",
    "selected_parents = balanced_subs[\"parent_image\"].nunique()\n",
    "print(f\"corresponding parent_image numbers：{selected_parents}\")\n",
    "\n",
    "balanced_subs.to_csv(\"balanced_tem_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfb479e-19ef-4cfe-84d6-1078708cddaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Read the caption_analysis_output.csv file and select rows with type in [\"nanomaterials\", \"crystallography\"]\n",
    "# 2. For the selected rows, copy the corresponding images:\n",
    "#    - From /home/ne6131039/Desktop/TEM_DATAS/LLaVA Dataset/PDF_images to /home/ne6131039/Desktop/Test_Dataset/PDF_images\n",
    "#    - From /home/ne6131039/Desktop/TEM_DATAS/LLaVA Dataset/TEM_images to /home/ne6131039/Desktop/Test_Dataset/TEM_images\n",
    "\n",
    "\n",
    "# Source paths\n",
    "pdf_src = \"/home/ne6131039/Desktop/TEM_DATAS/LLaVA Dataset/PDF_images\"\n",
    "tem_src = \"/home/ne6131039/Desktop/TEM_DATAS/LLaVA Dataset/TEM_images\"\n",
    "\n",
    "# Destination paths\n",
    "pdf_dst = \"/home/ne6131039/Desktop/Test_Dataset/PDF_images\"\n",
    "tem_dst = \"/home/ne6131039/Desktop/Test_Dataset/TEM_images\"\n",
    "\n",
    "os.makedirs(pdf_dst, exist_ok=True)\n",
    "os.makedirs(tem_dst, exist_ok=True)\n",
    "\n",
    "# Extract selected parent and sub image names from balanced dataset\n",
    "selected_parent_images = balanced_subs[\"parent_image\"].unique()\n",
    "selected_sub_images = balanced_subs[\"sub_image\"].tolist()\n",
    "\n",
    "# Copy parent PDF images based on selected_parent_images\n",
    "for parent_image in tqdm(selected_parent_images, desc=\"Copying parent\"):\n",
    "    src_pdf = os.path.join(pdf_src, parent_image)\n",
    "    dst_pdf = os.path.join(pdf_dst, parent_image)\n",
    "\n",
    "    if os.path.exists(src_pdf):\n",
    "        shutil.copy2(src_pdf, dst_pdf)\n",
    "        # print(f\"Copied PDF: {parent_image}\")\n",
    "    else:\n",
    "        print(f\"PDF not found: {src_pdf}\")\n",
    "\n",
    "# Copy sub TEM images from selected_sub_images\n",
    "for sub_image in tqdm(selected_sub_images, desc=\"Copying subs\"):\n",
    "    src_tem = os.path.join(tem_src, sub_image)\n",
    "    dst_tem = os.path.join(tem_dst, sub_image)\n",
    "\n",
    "    if os.path.exists(src_tem):\n",
    "        shutil.copy2(src_tem, dst_tem)\n",
    "        # print(f\"Copied TEM: {sub_image}\")\n",
    "    else:\n",
    "        print(f\"TEM not found: {src_tem}\")\n",
    "\n",
    "print(\"Balanced PDF & TEM images have been successfully copied.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c272cf4-3189-4e8d-bd11-64a1e1aa10b1",
   "metadata": {},
   "source": [
    "# upload the TEM_Dataset to hugging face \n",
    "> for next stage image input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1412e7-bbcd-40ce-ae23-64dbb3543119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Setting ====\n",
    "SOURCE_DIR = \"/home/ne6131039/Desktop/Test_Dataset/TEM_images\"  \n",
    "OUTPUT_DIR = \"/home/ne6131039/Desktop/Batch_upload_images/TEM_images\"  \n",
    "CSV_PATH = \"tem_batch_log.csv\"     \n",
    "BATCH_SIZE = 1000                      \n",
    "\n",
    "# ==== OUTPUT_DIR ====\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ==== collect all images ====\n",
    "valid_exts = {'.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff'}\n",
    "image_files = [f for f in sorted(os.listdir(SOURCE_DIR)) if Path(f).suffix.lower() in valid_exts]\n",
    "\n",
    "# ==== batch & copy ====\n",
    "batch_log = []\n",
    "for i, filename in enumerate(image_files):\n",
    "    batch_idx = i // BATCH_SIZE + 1\n",
    "    batch_folder = f\"TEM_Batch_{batch_idx:03d}\"\n",
    "    batch_path = os.path.join(OUTPUT_DIR, batch_folder)\n",
    "    os.makedirs(batch_path, exist_ok=True)\n",
    "\n",
    "    src_path = os.path.join(SOURCE_DIR, filename)\n",
    "    dst_path = os.path.join(batch_path, filename)\n",
    "    shutil.copy2(src_path, dst_path)\n",
    "\n",
    "    batch_log.append({\n",
    "        \"filename\": filename,\n",
    "        \"batch_folder\": f\"TEM_Batch_{batch_idx:03d}\"\n",
    "    })\n",
    "\n",
    "# ==== Write CSV ====\n",
    "with open(CSV_PATH, \"w\", newline='', encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=[\"filename\", \"batch_folder\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(batch_log)\n",
    "\n",
    "print(f\"Finish：dealed {len(image_files)} images，CSV record in：{CSV_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c440df9-bb69-42b1-a30a-678008e90b73",
   "metadata": {},
   "source": [
    "# Image data distill using GPT-4o for QA pairs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90797b2-4b02-45c6-846c-f5be0f7e699a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== PROMPT 定義 ======\n",
    "def build_system_prompt():\n",
    "    return (\n",
    "        \"\"\"\n",
    "            You are a scientific assistant constructing a visual question-answering (VQA) dataset based on Transmission Electron Microscopy (TEM) sub-images.\n",
    "            \n",
    "            You will be provided with:\n",
    "            - A caption describing the parent figure (for background context only; do not use it in answers)\n",
    "            - Several TEM sub-images cropped from the parent figure\n",
    "            - Predicted image type labels for each sub-image (for reference only; do not rely on them when answering)\n",
    "            \n",
    "            ---\n",
    "            \n",
    "            Your task is to generate high-quality question–answer (QA) pairs for each individual TEM sub-image, following the rules below.\n",
    "            \n",
    "            ### Step 1: Handle Blank or Empty Images\n",
    "            \n",
    "            First, check whether the sub-image contains any meaningful visual structure:\n",
    "            - Are there visible features such as particles, lattice fringes, contrast zones, porosity, or surface textures?\n",
    "            - Or is the image blank, blurry, purely noisy, or lacks discernible structures?\n",
    "            \n",
    "            → If the image appears blank or uninformative, return only one Level 0 QA pair:\n",
    "            \n",
    "            {\n",
    "              \"sub_image\": \"<filename>\",\n",
    "              \"question\": \"What can be seen in this image?\",\n",
    "              \"answer\": \"The image appears empty or lacks visible structures.\",\n",
    "              \"level\": 0\n",
    "            }\n",
    "            \n",
    "            Then skip further QA generation for this image.\n",
    "            \n",
    "            ---\n",
    "            \n",
    "            ### Step 2: QA Generation\n",
    "\n",
    "            If the image contains observable structures, you must generate following outputs:\n",
    "            question–answer (QA) pairs, divided into:\n",
    "            \n",
    "            - Level 1 (several questions): Basic classification\n",
    "            - Level 2 (several questions): Direct visual identification\n",
    "            - Level 3 (several questions): Visual reasoning\n",
    "            - 1 total description (Level 4): A comprehensive 3-paragraph summary\n",
    "            \n",
    "            #### Level 1 – Basic Classification (several QA)\n",
    "                Focus on identifying the image modality and type based on visible characteristics.\n",
    "                Do not rely on metadata or provided labels. \n",
    "            Example topics:  \n",
    "                - Modality: CTEM, HRTEM, STEM, diffraction\n",
    "                - Type: nanoparticle, lattice image, porous film, diffraction pattern            \n",
    "            Example:           \n",
    "                Q: What is the imaging modality used in this image?\n",
    "                A: The visible lattice fringes suggest this is an HRTEM image.\n",
    "                \n",
    "            #### Level 2 – Direct Visual Identification (several QA)\n",
    "                Focus only on clearly observable features, no reasoning.\n",
    "                Describe what you can count, measure, or visually confirm.\n",
    "            Example topics:\n",
    "                - Particle count\n",
    "                - Shape (e.g., spherical, rod-like)\n",
    "                - Contrast, porosity, texture\n",
    "                - Characteristic size (~100 nm)\n",
    "                - Spatial arrangement (aligned vs random)\n",
    "            Example:\n",
    "                Q: How many particles are visible in the image?\n",
    "                A: Approximately 20 particles are visible across the field of view.\n",
    "                \n",
    "            #### Level 3 – Visual Reasoning (several QA)\n",
    "                Use interpretation based on visual evidence, not speculation.\n",
    "                Infer implications or behaviors from observed features.\n",
    "            Example topics:\n",
    "                - Thickness estimation based on contrast\n",
    "                - Particle density or uniformity\n",
    "                - Self-assembly indicators\n",
    "                - Implications for material structure or properties\n",
    "            Example:\n",
    "                Q: Does the distribution of particles suggest uniformity?\n",
    "                A: The particles appear unevenly spaced, indicating non-uniform distribution and potential aggregation.\n",
    "                \n",
    "            #### Level 4 – Full Image Description (1 QA)\n",
    "                Summarize the image in 3 paragraphs, each reflecting insights from one level:\n",
    "                Paragraph 1: Basic classification (Level 1)\n",
    "                Paragraph 2: Visual identification (Level 2)\n",
    "                Paragraph 3: Reasoned interpretation (Level 3)\n",
    "                Use academic language and separate paragraphs with \\\\n\\\\n.\n",
    "            Example:\n",
    "                Question: Describe this image\n",
    "                Answer:\n",
    "                The image appears to be acquired using high-resolution transmission electron microscopy (HRTEM), as suggested by the presence of clear lattice fringes and high spatial resolution.\n",
    "                \n",
    "                Approximately 15 nearly spherical particles are distributed across the field. The contrast is sharp, and the particle sizes are relatively consistent, estimated to be under 100 nm. No obvious porosity is visible.\n",
    "                \n",
    "                The slight variation in contrast and particle overlap suggests differences in local thickness. The loosely clustered arrangement of particles may indicate weak self-assembly or aggregation effects during sample preparation.\n",
    "            ---\n",
    "            \n",
    "            ### Step 3: Use a Chain-of-Thought (CoT) process\n",
    "            \n",
    "            - Before selecting final QA items, you must first brainstorm at least 5 distinct candidate questions per level. Avoid overly vague or generic phrasing (e.g., \"What is the feature of this image?\").\n",
    "            \n",
    "            Then, select the best few per level based on:\n",
    "            - Topical diversity (covering different visual features)\n",
    "            - Clarity and precision of question and answer\n",
    "            - Scientific relevance and reasoning depth\n",
    "\n",
    "            ### Step 4: Output Format\n",
    "            \n",
    "            Return a **JSON list** of all QA pairs. Each item must include:\n",
    "            - `sub_image`: the filename of the sub-image\n",
    "            - `question`: a question that is answerable solely from the image\n",
    "            - `answer`: an accurate answer based on visual observation\n",
    "            - `level`: integer (0 = empty image, 1 = classification, 2 = recognition, 3 = reasoning, 4 = summary)\n",
    "            \n",
    "            ### Important Rules:\n",
    "            - Do NOT use parent captions or predicted labels in answers (they are for context only).\n",
    "            - Do NOT mention material-specific names (e.g., Cu, Au) unless they are visibly labeled in the image.\n",
    "            - Use general scientific terms such as \"the material\", \"the nanoparticles\", or \"the structure\".\n",
    "            - Each sub-image must be treated independently. Do NOT compare it to other images.\n",
    "            - All questions and answers must be **answerable based only on the visual content** of the sub-image.\n",
    "            - Return only a valid JSON array of objects. No explanation, no formatting, no markdown.\n",
    "            - Do not include any headings, explanations, markdown, or extra text.\n",
    "            - Return a valid raw JSON array only. Do not wrap in triple quotes or code fences.\n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef92ca1-ee21-4943-a1bb-0dad7406bdba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ====== Initialize OpenAI client ======\n",
    "client = OpenAI(api_key=openai_key)\n",
    "\n",
    "# ====== Base URL for image access on Hugging Face ======\n",
    "IMAGE_BASE_URL = \"https://huggingface.co/datasets/LabSmart/TEM_Dataset/resolve/main\"\n",
    "\n",
    "# ====== File paths ======\n",
    "OUTPUT_CSV = \"gpt_reply_log.csv\"        \n",
    "CAPTION_CSV = \"caption_analysis_output.csv\"\n",
    "TEM_CSV = \"balanced_tem_data.csv\"\n",
    "OUTPUT_JSONL = \"distilled_llava_vqa.jsonl\"\n",
    "FAILED_LIST = \"failed_images.txt\"\n",
    "PDF_BATCH_LOG = \"pdf_batch_log.csv\"\n",
    "TEM_BATCH_LOG = \"tem_batch_log.csv\"\n",
    "\n",
    "# ====== Load metadata and batch logs ======\n",
    "df_caption = pd.read_csv(CAPTION_CSV)\n",
    "df_tem = pd.read_csv(TEM_CSV)\n",
    "df_pdf_batch = pd.read_csv(PDF_BATCH_LOG)\n",
    "df_tem_batch = pd.read_csv(TEM_BATCH_LOG)\n",
    "\n",
    "# ====== Map image filename to batch folder ======\n",
    "parent_to_batch = dict(zip(df_pdf_batch[\"filename\"], df_pdf_batch[\"batch_folder\"]))\n",
    "sub_to_batch = dict(zip(df_tem_batch[\"filename\"], df_tem_batch[\"batch_folder\"]))\n",
    "\n",
    "# ====== Filter relevant captions ======\n",
    "df_caption = df_caption[df_caption[\"caption_type\"].isin([\"nanomaterials\", \"crystallography\"])]\n",
    "df = df_tem.merge(\n",
    "    df_caption[[\"image_name\", \"caption_content\"]],\n",
    "    left_on=\"parent_image\",\n",
    "    right_on=\"image_name\",\n",
    "    how=\"left\"\n",
    ")\n",
    "df.drop(columns=[\"image_name\"], inplace=True)\n",
    "assert \"caption_content\" in df.columns, \"fail to pair caption\"\n",
    "\n",
    "# ====== Group sub-images by their parent image ======\n",
    "grouped = df.groupby(\"parent_image\")\n",
    "MAX_GROUPS = 22000\n",
    "total_calls = 0\n",
    "error_images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f70fce-7355-462b-b39a-275c8f9f5557",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ====== Process each parent image and its sub-images ======\n",
    "with open(OUTPUT_CSV, \"a\", encoding=\"utf-8\", newline=\"\") as f_log, open(FAILED_LIST, \"a\") as f_fail:\n",
    "    writer = csv.DictWriter(f_log, fieldnames=[\"parent_image\", \"sub_image_ids\", \"reply\"])\n",
    "    if f_log.tell() == 0:\n",
    "        writer.writeheader()\n",
    "\n",
    "    for i, (parent_image, group) in enumerate(grouped):\n",
    "        if i < 20000:\n",
    "            continue\n",
    "        if i >= MAX_GROUPS:\n",
    "            break\n",
    "\n",
    "        print(f\"Dealing with {i+1} th parent image: {parent_image}\\n\")\n",
    "        try:\n",
    "            caption = group.iloc[0][\"caption_content\"]\n",
    "            parent_batch = parent_to_batch.get(parent_image)\n",
    "            if not parent_batch:\n",
    "                raise ValueError(f\"Missing parent batch for: {parent_image}\")\n",
    "            parent_url = f\"{IMAGE_BASE_URL}/{parent_batch}/{parent_image}\"\n",
    "            parent_image_input = {\"type\": \"image_url\", \"image_url\": {\"url\": parent_url}}\n",
    "\n",
    "            sub_image_inputs = []\n",
    "            user_prompts = []\n",
    "            sub_image_ids = []\n",
    "            # List including explicit text reference to image name\n",
    "            sub_image_with_text = []\n",
    "\n",
    "            for _, row in group.iterrows():\n",
    "                sub_image = row[\"sub_image\"]\n",
    "                tem_type = row[\"TEM_type\"]\n",
    "                try:\n",
    "                    sub_batch = sub_to_batch.get(sub_image)\n",
    "                    if not sub_batch:\n",
    "                        raise ValueError(f\"Missing sub batch for: {sub_image}\")\n",
    "                    sub_url = f\"{IMAGE_BASE_URL}/{sub_batch}/{sub_image}\"\n",
    "                    sub_image_inputs.append({\"type\": \"image_url\", \"image_url\": {\"url\": sub_url}})\n",
    "                    user_prompts.append(f\"Sub-image: {sub_image} (predicted TEM type: {tem_type})\")\n",
    "                    sub_image_ids.append(sub_image)\n",
    "                    \n",
    "                    # Include filename explicitly before each image\n",
    "                    sub_image_with_text.append({\"type\": \"text\", \"text\": f\"Filename: {sub_image}\"})\n",
    "                    sub_image_with_text.append({\"type\": \"image_url\", \"image_url\": {\"url\": sub_url}})\n",
    "                except Exception as e:\n",
    "                    error_msg = f\"{sub_image} | URL error: {e}\"\n",
    "                    print(error_msg)\n",
    "                    f_fail.write(error_msg + \"\\n\")\n",
    "                    error_images.append(sub_image)\n",
    "\n",
    "            if not sub_image_inputs:\n",
    "                print(f\" Skip：{parent_image} , no sub-images\")\n",
    "                f_fail.write(f\"{parent_image} | No sub-images\\n\")\n",
    "                continue\n",
    "\n",
    "            if pd.isna(caption):\n",
    "                caption = \"\"\n",
    "\n",
    "            # Construct user prompt clearly identifying all image filenames\n",
    "            user_prompt = (\n",
    "                f\"The following images will be shown to you:\\n\"\n",
    "                f\"Parent image filename: {parent_image}\\n\"\n",
    "                f\"Caption: {caption.strip()}\\n\\n\"\n",
    "                + \"\\n\".join(user_prompts) +\n",
    "                \"\\n\\nPlease generate multiple high-quality VQA (Visual Question Answering) pairs for each sub-image. \"\n",
    "                \"Each VQA pair should include a meaningful question and answer that is visually grounded in the image content. \"\n",
    "                \"Return your answer as a JSON list with the fields: sub_image, question, answer, level.\"\n",
    "            )\n",
    "\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": build_system_prompt()},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": user_prompt},\n",
    "                        {\"type\": \"text\", \"text\": f\"Filename: {parent_image}\"},\n",
    "                        parent_image_input,\n",
    "                        *sub_image_with_text\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            # Call GPT model\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o\",\n",
    "                    messages=messages,\n",
    "                    temperature=0.7,\n",
    "                )\n",
    "                reply = response.choices[0].message.content.strip()\n",
    "            except:\n",
    "                try:\n",
    "                    time.sleep(30)\n",
    "                    response = client.chat.completions.create(\n",
    "                        model=\"gpt-4o\",\n",
    "                        messages=messages,\n",
    "                        temperature=0.7,\n",
    "                    )\n",
    "                    reply = response.choices[0].message.content.strip()\n",
    "                except:\n",
    "                    reply = \"\"\n",
    "                \n",
    "            total_calls += 1\n",
    "\n",
    "            # Log result\n",
    "            writer.writerow({\n",
    "                \"parent_image\": parent_image,\n",
    "                \"sub_image_ids\": \";\".join(sub_image_ids),\n",
    "                \"reply\": reply\n",
    "            })\n",
    "\n",
    "            # Explicitly delete used vars to reduce memory pressure\n",
    "            del sub_image_inputs, sub_image_ids, sub_image_with_text, user_prompts, messages, reply, response\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = f\"{parent_image} | GPT API error: {e}\"\n",
    "            print(\"❌\", error_msg)\n",
    "            traceback.print_exc()\n",
    "            for sub, _ in group[[\"sub_image\"]].itertuples(index=False):\n",
    "                f_fail.write(f\"{sub} | GPT error: {e}\\n\")\n",
    "                error_images.append(sub)\n",
    "            time.sleep(1)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "# ====== Final report ======\n",
    "print(f\"\\n✅ Finish first phase，sent {total_calls}  times GPT request。\")\n",
    "print(f\"❌ Failed images total {len(error_images)} ，record to {FAILED_LIST}。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ocr_env)",
   "language": "python",
   "name": "ocr_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
