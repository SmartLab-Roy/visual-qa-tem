{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf7c4e96-cd36-4673-bd16-e1af13757422",
   "metadata": {},
   "source": [
    "# Use gpt-3.5-turbo to combine the pix2text & easy_ocr outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ccd9b9-0986-47f8-8be8-45de7f94b8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === kernel system ===\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import shutil\n",
    "import traceback\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "# === packages ===\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from openai import OpenAI\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0126c17-6034-474a-b397-cc2aca8a4f1c",
   "metadata": {},
   "source": [
    "# Load api keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e67f5e1-dbfb-4e66-a5a7-54a8c8148240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_keys(path):\n",
    "    keys = {}\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            if \"=\" in line:\n",
    "                k, v = line.strip().split(\"=\", 1)\n",
    "                keys[k] = v\n",
    "    return keys\n",
    "\n",
    "keys = load_keys(\"/home/ne6131039/Desktop/Confidential_Key.txt\")\n",
    "openai_key = keys['OPENAI_KEY']\n",
    "huggingface_write_key = keys['HUGGINGFACE_WRITE_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9facb77d-f59b-4723-af58-c6c016a7184e",
   "metadata": {},
   "source": [
    "# Merge two type of text and label caption_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffa26da-5353-4900-b0a2-47c90af3193d",
   "metadata": {},
   "source": [
    "### System prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f20f25-9209-433a-8d8d-1e446ede7bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt construction logic omitted for proprietary reasons\n",
    "# Contact authors for detailed evaluation methodology\n",
    "system_prompt = \"\"\"\n",
    "Creates prompt for GPT to process and classify scientific captions.\n",
    "    \n",
    "The prompt instructs GPT to:\n",
    "- Combine OCR results from Pix2Text (LaTeX) and EasyOCR (plain text)\n",
    "- Reconstruct coherent, readable scientific captions\n",
    "- Classify content into predefined categories\n",
    "- Apply proper scientific formatting (Unicode superscripts, preserve formulas)\n",
    "- Output in structured format: caption_content + caption_type\n",
    "\n",
    "Args:\n",
    "    pix2text_output (str): LaTeX-formatted OCR result\n",
    "    easyocr_output (str): Plain text OCR result\n",
    "    \n",
    "Returns:\n",
    "    str: Formatted caption processing prompt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d806e1-e497-46b8-8666-9524e8fa32db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=openai_key)  \n",
    "\n",
    "# Constants\n",
    "INPUT_CSV = \"../caption_analysis/ocr_output.csv\"\n",
    "OUTPUT_CSV = \"caption_analysis_output.csv\"\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "#Initialize index \n",
    "START_INDEX = 0  \n",
    "\n",
    "# Load input\n",
    "df = pd.read_csv(INPUT_CSV).fillna(\"\")\n",
    "total_rows = len(df)\n",
    "# total_rows = 40000\n",
    "\n",
    "# Check how many already processed\n",
    "if os.path.exists(OUTPUT_CSV):\n",
    "    output_df = pd.read_csv(OUTPUT_CSV)\n",
    "    processed_images = set(output_df[\"image_name\"])\n",
    "else:\n",
    "    output_df = pd.DataFrame(columns=[\"image_name\", \"caption_content\", \"caption_type\"])\n",
    "    processed_images = set()\n",
    "\n",
    "# Process in batches\n",
    "for idx in range(START_INDEX, total_rows):\n",
    "    row = df.iloc[idx]\n",
    "    image_name = row[\"image_name\"]\n",
    "\n",
    "    # Skip if already processed\n",
    "    if image_name in processed_images:\n",
    "        continue\n",
    "\n",
    "    print(f\" Processing {idx+1}/{total_rows}: {image_name}\")\n",
    "\n",
    "    combined_text = f\"\"\"\n",
    "[Pix2Text OCR]:\n",
    "{row['pix2text_ocr'].strip()}\n",
    "\n",
    "[EasyOCR OCR]:\n",
    "{row['easyocr_ocr'].strip()}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": combined_text}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            max_tokens=4000\n",
    "        )\n",
    "\n",
    "        reply = response.choices[0].message.content.strip()\n",
    "        lines = reply.splitlines()\n",
    "        \n",
    "        caption_content = \"\"\n",
    "        caption_type = \"\"\n",
    "        \n",
    "        for line in lines:\n",
    "            if line.startswith(\"caption_content:\"):\n",
    "                caption_content = line.replace(\"caption_content:\", \"\").strip()\n",
    "            elif line.startswith(\"caption_type:\"):\n",
    "                caption_type = line.replace(\"caption_type:\", \"\").strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        caption_content = f\"[ERROR] {e}\"\n",
    "        caption_type = \"unknown\"\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Save result\n",
    "    new_row = pd.DataFrame([{\n",
    "        \"image_name\": image_name,\n",
    "        \"caption_content\": caption_content,\n",
    "        \"caption_type\": caption_type\n",
    "    }])\n",
    "\n",
    "    new_row.to_csv(OUTPUT_CSV, mode=\"a\", header=not os.path.exists(OUTPUT_CSV), index=False)\n",
    "\n",
    "    # Optional delay to avoid rate limits\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print (\"Processing Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c440df9-bb69-42b1-a30a-678008e90b73",
   "metadata": {},
   "source": [
    "# Image data distill using GPT-4o for QA pairs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90797b2-4b02-45c6-846c-f5be0f7e699a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== PROMPT 定義 ======\n",
    "def build_system_prompt():\n",
    "    return (\n",
    "# Prompt construction logic omitted for proprietary reasons\n",
    "# Contact authors for detailed evaluation methodology\n",
    "        \"\"\"\n",
    "            Constructs prompt for GPT to generate TEM image VQA datasets.\n",
    "    \n",
    "            The prompt instructs GPT to:\n",
    "            - Act as a scientific assistant for TEM image analysis\n",
    "            - Generate question-answer pairs by task type for each sub-image:\n",
    "              * Classification tasks: Imaging modality, image type identification\n",
    "              * Recognition tasks: Observable features and measurements\n",
    "              * Analysis tasks: Visual reasoning and interpretations\n",
    "              * Description tasks: Comprehensive image summaries\n",
    "            - Handle empty/blank images appropriately\n",
    "            - Use Chain-of-Thought process for question generation\n",
    "            - Return structured JSON output with specified format\n",
    "            - Follow scientific accuracy guidelines (visual evidence only)\n",
    "            \n",
    "            Key constraints:\n",
    "            - Don't use parent captions or labels in answers\n",
    "            - Avoid material-specific names unless visible\n",
    "            - Treat each sub-image independently\n",
    "            - Generate multiple QA pairs per task type for diversity\n",
    "            \n",
    "            Args:\n",
    "                caption (str): Parent figure caption (context only)\n",
    "                sub_images (list): List of sub-image filenames\n",
    "                predicted_labels (list): Predicted labels (reference only)\n",
    "                \n",
    "            Returns:\n",
    "                str: Formatted VQA generation prompt\n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef92ca1-ee21-4943-a1bb-0dad7406bdba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ====== Initialize OpenAI client ======\n",
    "client = OpenAI(api_key=openai_key)\n",
    "\n",
    "# ====== Base URL for image access on Hugging Face ======\n",
    "IMAGE_BASE_URL = \"https://huggingface.co/datasets/LabSmart/TEM_Dataset/resolve/main\"\n",
    "\n",
    "# ====== File paths ======\n",
    "OUTPUT_CSV = \"gpt_reply_log.csv\"        \n",
    "CAPTION_CSV = \"caption_analysis_output.csv\"\n",
    "TEM_CSV = \"balanced_tem_data.csv\"\n",
    "OUTPUT_JSONL = \"distilled_llava_vqa.jsonl\"\n",
    "FAILED_LIST = \"failed_images.txt\"\n",
    "PDF_BATCH_LOG = \"pdf_batch_log.csv\"\n",
    "TEM_BATCH_LOG = \"tem_batch_log.csv\"\n",
    "\n",
    "# ====== Load metadata and batch logs ======\n",
    "df_caption = pd.read_csv(CAPTION_CSV)\n",
    "df_tem = pd.read_csv(TEM_CSV)\n",
    "df_pdf_batch = pd.read_csv(PDF_BATCH_LOG)\n",
    "df_tem_batch = pd.read_csv(TEM_BATCH_LOG)\n",
    "\n",
    "# ====== Map image filename to batch folder ======\n",
    "parent_to_batch = dict(zip(df_pdf_batch[\"filename\"], df_pdf_batch[\"batch_folder\"]))\n",
    "sub_to_batch = dict(zip(df_tem_batch[\"filename\"], df_tem_batch[\"batch_folder\"]))\n",
    "\n",
    "# ====== Filter relevant captions ======\n",
    "df_caption = df_caption[df_caption[\"caption_type\"].isin([\"nanomaterials\", \"crystallography\"])]\n",
    "df = df_tem.merge(\n",
    "    df_caption[[\"image_name\", \"caption_content\"]],\n",
    "    left_on=\"parent_image\",\n",
    "    right_on=\"image_name\",\n",
    "    how=\"left\"\n",
    ")\n",
    "df.drop(columns=[\"image_name\"], inplace=True)\n",
    "assert \"caption_content\" in df.columns, \"fail to pair caption\"\n",
    "\n",
    "# ====== Group sub-images by their parent image ======\n",
    "grouped = df.groupby(\"parent_image\")\n",
    "MAX_GROUPS = 22000\n",
    "total_calls = 0\n",
    "error_images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f70fce-7355-462b-b39a-275c8f9f5557",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ====== Process each parent image and its sub-images ======\n",
    "with open(OUTPUT_CSV, \"a\", encoding=\"utf-8\", newline=\"\") as f_log, open(FAILED_LIST, \"a\") as f_fail:\n",
    "    writer = csv.DictWriter(f_log, fieldnames=[\"parent_image\", \"sub_image_ids\", \"reply\"])\n",
    "    if f_log.tell() == 0:\n",
    "        writer.writeheader()\n",
    "\n",
    "    for i, (parent_image, group) in enumerate(grouped):\n",
    "        if i < 20000:\n",
    "            continue\n",
    "        if i >= MAX_GROUPS:\n",
    "            break\n",
    "\n",
    "        print(f\"Dealing with {i+1} th parent image: {parent_image}\\n\")\n",
    "        try:\n",
    "            caption = group.iloc[0][\"caption_content\"]\n",
    "            parent_batch = parent_to_batch.get(parent_image)\n",
    "            if not parent_batch:\n",
    "                raise ValueError(f\"Missing parent batch for: {parent_image}\")\n",
    "            parent_url = f\"{IMAGE_BASE_URL}/{parent_batch}/{parent_image}\"\n",
    "            parent_image_input = {\"type\": \"image_url\", \"image_url\": {\"url\": parent_url}}\n",
    "\n",
    "            sub_image_inputs = []\n",
    "            user_prompts = []\n",
    "            sub_image_ids = []\n",
    "            # List including explicit text reference to image name\n",
    "            sub_image_with_text = []\n",
    "\n",
    "            for _, row in group.iterrows():\n",
    "                sub_image = row[\"sub_image\"]\n",
    "                tem_type = row[\"TEM_type\"]\n",
    "                try:\n",
    "                    sub_batch = sub_to_batch.get(sub_image)\n",
    "                    if not sub_batch:\n",
    "                        raise ValueError(f\"Missing sub batch for: {sub_image}\")\n",
    "                    sub_url = f\"{IMAGE_BASE_URL}/{sub_batch}/{sub_image}\"\n",
    "                    sub_image_inputs.append({\"type\": \"image_url\", \"image_url\": {\"url\": sub_url}})\n",
    "                    user_prompts.append(f\"Sub-image: {sub_image} (predicted TEM type: {tem_type})\")\n",
    "                    sub_image_ids.append(sub_image)\n",
    "                    \n",
    "                    # Include filename explicitly before each image\n",
    "                    sub_image_with_text.append({\"type\": \"text\", \"text\": f\"Filename: {sub_image}\"})\n",
    "                    sub_image_with_text.append({\"type\": \"image_url\", \"image_url\": {\"url\": sub_url}})\n",
    "                except Exception as e:\n",
    "                    error_msg = f\"{sub_image} | URL error: {e}\"\n",
    "                    print(error_msg)\n",
    "                    f_fail.write(error_msg + \"\\n\")\n",
    "                    error_images.append(sub_image)\n",
    "\n",
    "            if not sub_image_inputs:\n",
    "                print(f\" Skip：{parent_image} , no sub-images\")\n",
    "                f_fail.write(f\"{parent_image} | No sub-images\\n\")\n",
    "                continue\n",
    "\n",
    "            if pd.isna(caption):\n",
    "                caption = \"\"\n",
    "\n",
    "            # Construct user prompt clearly identifying all image filenames\n",
    "            user_prompt = (\n",
    "                f\"The following images will be shown to you:\\n\"\n",
    "                f\"Parent image filename: {parent_image}\\n\"\n",
    "                f\"Caption: {caption.strip()}\\n\\n\"\n",
    "                + \"\\n\".join(user_prompts) +\n",
    "                \"\\n\\nPlease generate multiple high-quality VQA (Visual Question Answering) pairs for each sub-image. \"\n",
    "                \"Each VQA pair should include a meaningful question and answer that is visually grounded in the image content. \"\n",
    "                \"Return your answer as a JSON list with the fields: sub_image, question, answer, level.\"\n",
    "            )\n",
    "\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": build_system_prompt()},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": user_prompt},\n",
    "                        {\"type\": \"text\", \"text\": f\"Filename: {parent_image}\"},\n",
    "                        parent_image_input,\n",
    "                        *sub_image_with_text\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            # Call GPT model\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o\",\n",
    "                    messages=messages,\n",
    "                    temperature=0.7,\n",
    "                )\n",
    "                reply = response.choices[0].message.content.strip()\n",
    "            except:\n",
    "                try:\n",
    "                    time.sleep(30)\n",
    "                    response = client.chat.completions.create(\n",
    "                        model=\"gpt-4o\",\n",
    "                        messages=messages,\n",
    "                        temperature=0.7,\n",
    "                    )\n",
    "                    reply = response.choices[0].message.content.strip()\n",
    "                except:\n",
    "                    reply = \"\"\n",
    "                \n",
    "            total_calls += 1\n",
    "\n",
    "            # Log result\n",
    "            writer.writerow({\n",
    "                \"parent_image\": parent_image,\n",
    "                \"sub_image_ids\": \";\".join(sub_image_ids),\n",
    "                \"reply\": reply\n",
    "            })\n",
    "\n",
    "            # Explicitly delete used vars to reduce memory pressure\n",
    "            del sub_image_inputs, sub_image_ids, sub_image_with_text, user_prompts, messages, reply, response\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = f\"{parent_image} | GPT API error: {e}\"\n",
    "            print(\"❌\", error_msg)\n",
    "            traceback.print_exc()\n",
    "            for sub, _ in group[[\"sub_image\"]].itertuples(index=False):\n",
    "                f_fail.write(f\"{sub} | GPT error: {e}\\n\")\n",
    "                error_images.append(sub)\n",
    "            time.sleep(1)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "# ====== Final report ======\n",
    "print(f\"\\n✅ Finish first phase，sent {total_calls}  times GPT request。\")\n",
    "print(f\"❌ Failed images total {len(error_images)} ，record to {FAILED_LIST}。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ocr_env)",
   "language": "python",
   "name": "ocr_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
