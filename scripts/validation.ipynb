{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21a1cca0-089c-4900-9619-405009a62227",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c5e380-93c8-4581-abd9-0f4f10882739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import gc\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine learning and NLP\n",
    "import evaluate\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoModel, AutoTokenizer, pipeline\n",
    "\n",
    "# API clients\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a79be05-6ccf-4692-b5f1-cc70717e627b",
   "metadata": {},
   "source": [
    "# The main scoring section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a20e06a-9b15-4147-ae57-44ab85e19a01",
   "metadata": {},
   "source": [
    "## Set up for GPT as judge to do the human-like scoring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c99b10-bfbb-4901-9479-df352a1a4f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從 txt 檔讀取 API 金鑰\n",
    "with open(\"/home/NE6131039/Desktop/Confidential_Key.txt\", \"r\") as f:\n",
    "    api_key = f.read().strip()\n",
    "\n",
    "# 初始化新版 OpenAI client\n",
    "client = OpenAI(api_key=api_key)\n",
    "GPT_MODEL = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafa4f82-a38b-4705-a3c7-4cb9d266b7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gpt_score_prompt(prediction, reference, question):\n",
    "    return f\"\"\"\n",
    "You are an expert in Transmission Electron Microscopy (TEM) image analysis.\n",
    "Evaluate how well the predicted answer matches the reference answer for the given question.\n",
    "\n",
    "### Evaluation Criteria:\n",
    "1. **Factual Accuracy (50%)**: Is the core information scientifically correct?\n",
    "   - For numerical values: Allow reasonable measurement tolerances (±10-20%)\n",
    "   - For counts: Accept reasonable approximations within scientific context\n",
    "   - For descriptions: Focus on scientific validity over exact phrasing\n",
    "   \n",
    "2. **Semantic Completeness (30%)**: Does the prediction fully address the question?\n",
    "   - All key aspects mentioned in reference should be covered\n",
    "   - Partial answers receive proportional scores\n",
    "   \n",
    "3. **Technical Precision (20%)**: Are scientific terms and concepts used correctly?\n",
    "   - Proper TEM terminology and scientific language\n",
    "   - Accurate use of materials science concepts\n",
    "\n",
    "### Scoring Guidelines:\n",
    "- **0.90-1.00**: Scientifically accurate, complete, and technically precise\n",
    "- **0.70-0.89**: Mostly correct with minor inaccuracies or omissions\n",
    "- **0.50-0.69**: Partially correct but missing important information\n",
    "- **0.30-0.49**: Some relevant content but significant errors or gaps\n",
    "- **0.00-0.29**: Largely incorrect, irrelevant, or completely missing key information\n",
    "\n",
    "### Important Notes:\n",
    "- Focus on scientific substance over linguistic similarity\n",
    "- Consider context-appropriate terminology variations\n",
    "- Evaluate based on TEM domain expertise\n",
    "\n",
    "**Question:** {question}\n",
    "**Reference:** {reference}\n",
    "**Prediction:** {prediction}\n",
    "\n",
    "**Provide a numerical score with exactly 2 decimal places (format: X.XX, range 0.00-1.00):**\n",
    "**Examples: 0.85, 0.73, 0.91, 0.42**\n",
    "\n",
    "retrun only a float score\n",
    "\"\"\"\n",
    "\n",
    "def gpt_score(prediction, reference, question):\n",
    "    prompt = build_gpt_score_prompt(prediction, reference, question)\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=GPT_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0,\n",
    "        )\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # 更強健的數字匹配，支援兩位小數\n",
    "        match = re.search(r\"(\\d+(?:\\.\\d{1,2})?)\", content)\n",
    "        if match:\n",
    "            score = float(match.group(1))\n",
    "            # 確保在有效範圍內並格式化為兩位小數\n",
    "            score = min(1.0, max(0.0, score))\n",
    "            return round(score, 2)  # 確保兩位小數\n",
    "        \n",
    "        # 如果沒有匹配到，嘗試找其他可能的格式\n",
    "        print(f\"[GPT PARSE WARNING] Could not parse score from: {content}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[GPT ERROR] {e}\")\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d2cda8-c973-4c36-a4ce-a0e88d5e34f7",
   "metadata": {},
   "source": [
    "## Load in files & scoring metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7a857e-78a7-4b2e-a715-94df5749ac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answers = [\n",
    "#     \"train_Classification_pretrain_7b_predict_final_scored.csv\",\n",
    "#     \"train_Recognition_pretrain_7b_predict_final_scored.csv\",\n",
    "#     \"train_Reasoning_pretrain_7b_predict_final_scored.csv\",\n",
    "#     \"train_Summary_pretrain_7b_predict_final_scored.csv\"\n",
    "# ]\n",
    "answers = [\n",
    "    \"pretrain_val_predict.csv\",\n",
    "    \"finetune_no_curriculum_val_predict.csv\",\n",
    "    # \"finetune_curriculum_val_predict.csv\",\n",
    "    \"finetune_curriculum_final_val_predict.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90950608-c607-4da3-bb0c-67254a312a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = evaluate.load(\"bleu\")\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# sbert_A = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "sbert = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "device = torch.device(\"cuda:2\")  # 指定 GPU 1\n",
    "# sbert_A = sbert_A.to(device)\n",
    "sbert = sbert.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44228b19-8f66-4309-a081-2d8831dc6702",
   "metadata": {},
   "source": [
    "## all data scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e552b3-26a3-4127-bad3-a63deead3957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Main Loop ====\n",
    "for answer in answers:\n",
    "    df = pd.read_csv(answer)\n",
    "    \n",
    "    sample_df = df.groupby('type').sample(n=2500, random_state=42)\n",
    "    df = sample_df.reset_index(drop=True)\n",
    "    \n",
    "    references = df[\"expected\"].astype(str).tolist()\n",
    "    predictions = df[\"predicted\"].astype(str).tolist()\n",
    "    questions = df[\"question\"].astype(str).tolist()\n",
    "\n",
    "    # =======================\n",
    "    #        GPT_SCORE\n",
    "    # =======================\n",
    "    gpt_scores = []\n",
    "    for pred, ref,que in tqdm(zip(predictions, references,questions), total=len(predictions), desc=\"GPT Scoring\"):\n",
    "        gpt_scores.append(gpt_score(pred, ref,que))\n",
    "\n",
    "    df[\"gpt_score\"] = gpt_scores\n",
    "    \n",
    "    lexical_scores = []\n",
    "    bleu_combined_scores = []\n",
    "    rouge_combined_scores = []\n",
    "    meteor_scores = []\n",
    "    # =======================\n",
    "    #        LEXICAL\n",
    "    # =======================\n",
    "    for pred, ref in tqdm(zip(predictions, references), total=len(predictions)):\n",
    "        try:\n",
    "            \n",
    "            # BLEU-1 to BLEU-4 (get all max_order BLEU scores)\n",
    "            bleu_scores_all = bleu.compute(predictions=[pred], references=[[ref]], max_order=4, smooth=True)\n",
    "            bleu1 = bleu_scores_all.get(\"precisions\", [0, 0, 0, 0])[0]\n",
    "            bleu2 = bleu_scores_all.get(\"precisions\", [0, 0, 0, 0])[1]\n",
    "            bleu3 = bleu_scores_all.get(\"precisions\", [0, 0, 0, 0])[2]\n",
    "            bleu4 = bleu_scores_all.get(\"precisions\", [0, 0, 0, 0])[3]\n",
    "            \n",
    "            bleu_combined  = 0.4*bleu1+0.3*bleu2+0.2*bleu3+0.1*bleu4\n",
    "            \n",
    "            #METEOR \n",
    "            meteor_score = meteor.compute(predictions=[pred], references=[ref])[\"meteor\"]\n",
    "\n",
    "            #ROUGE\n",
    "            rouge_score = rouge.compute(predictions=[pred], references=[ref])\n",
    "            rouge_1 = rouge_score.get(\"rouge1\", 0.0)\n",
    "            rouge_2 = rouge_score.get(\"rouge2\", 0.0)\n",
    "            rouge_l = rouge_score.get(\"rougeL\", 0.0)\n",
    "            rouge_lsum = rouge_score.get(\"rougeLsum\", 0.0)\n",
    "            \n",
    "            rouge_combined = (rouge_1 + rouge_2 + ((rouge_l + rouge_lsum) / 2)) / 3\n",
    "\n",
    "            score = (\n",
    "                0.3 * rouge_combined +\n",
    "                0.2 * bleu_combined +\n",
    "                0.5 * meteor_score\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error on sample: {e}\")\n",
    "            score = 0.0\n",
    "            \n",
    "        # Append metrics\n",
    "        bleu_combined_scores.append(round(min(max(bleu_combined, 0.0), 1.0), 4))\n",
    "        rouge_combined_scores.append(round(min(max(rouge_combined, 0.0), 1.0), 4))\n",
    "        meteor_scores.append(round(min(max(meteor_score, 0.0), 1.0), 4))\n",
    "        lexical_scores.append(round(min(max(score, 0.0), 1.0), 4))\n",
    "        \n",
    "    df[\"bleu_scores\"] = bleu_combined_scores\n",
    "    df[\"rouge_scores\"] = rouge_combined_scores\n",
    "    df[\"meteor_scores\"] = meteor_scores\n",
    "    df[\"lexical_scores\"] = lexical_scores\n",
    "    \n",
    "\n",
    "    # =======================\n",
    "    #        SEMANTIC\n",
    "    # =======================\n",
    "    bert_scores = []\n",
    "    sbert_scores = []\n",
    "    semantic_scores = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing {answer}\"):\n",
    "        pred = str(row[\"predicted\"])\n",
    "        ref = str(row[\"expected\"])\n",
    "\n",
    "        # --- BERTScore ---\n",
    "        try:\n",
    "            bert_result = bertscore.compute(\n",
    "                predictions=[pred],\n",
    "                references=[ref],\n",
    "                lang=\"en\",\n",
    "                model_type=\"microsoft/deberta-xlarge-mnli\",\n",
    "                device=\"cuda:3\"\n",
    "            )\n",
    "            bert_f1 = bert_result[\"f1\"][0]\n",
    "        except Exception as e:\n",
    "            print(f\"BERTScore error: {e}\")\n",
    "            bert_f1 = 0.0\n",
    "\n",
    "        # --- SBERT cosine similarity ---\n",
    "        try:\n",
    "            emb_pred = sbert.encode(pred, convert_to_tensor=True, device=\"cuda:2\")\n",
    "            emb_ref = sbert.encode(ref, convert_to_tensor=True, device=\"cuda:2\")\n",
    "            sim = util.cos_sim(emb_pred, emb_ref).item()\n",
    "        except Exception as e:\n",
    "            print(f\"SBERT error: {e}\")\n",
    "            sim = 0.0\n",
    "\n",
    "        # Average semantic score\n",
    "        avg = (bert_f1 + sim) / 2\n",
    "\n",
    "        # Accumulate\n",
    "        bert_scores.append(round(min(max(bert_f1, 0.0), 1.0), 4))\n",
    "        sbert_scores.append(round(min(max(sim, 0.0), 1.0), 4))\n",
    "        semantic_scores.append(round(min(max(avg, 0.0), 1.0), 4))\n",
    "\n",
    "        # Optional: cleanup per-row\n",
    "        del emb_pred, emb_ref\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # Save\n",
    "    df[\"bert_scores\"] = bert_scores\n",
    "    df[\"sbert_scores\"] = sbert_scores\n",
    "    df[\"semantic_scores\"] = semantic_scores\n",
    "   \n",
    "    # # =======================\n",
    "    # #     FINAL SCORE\n",
    "    # # =======================\n",
    "    lexical_np = np.array(lexical_scores)\n",
    "    semantic_np = np.array(semantic_scores)\n",
    "\n",
    "    #average\n",
    "    final_scores = 0.5 * lexical_np +0.5* semantic_np\n",
    "    df[\"final_scores\"] = final_scores\n",
    "    \n",
    "    # df.drop(columns=[\"image\", \"question\", \"expected\", \"predicted\"], inplace=True)\n",
    "    df.to_csv(answer.replace(\".csv\", \"_scored.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3754cb3c-14c2-40d3-a37e-5af917d216b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# deal with difficulity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c303f27-4aa1-492b-9179-8e6e5f7e5f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_final_average_scores = []\n",
    "for answer in answers:\n",
    "    df = pd.read_csv(answer)\n",
    "    average_scores = np.array(df['final_score_average'])\n",
    "    all_final_average_scores.append(average_scores)\n",
    "\n",
    "# 設定標籤和顏色\n",
    "colors = [\"steelblue\", \"orange\", \"green\", \"purple\"]\n",
    "labels = [\"Image Classification\", \"Microstructural Feature Detection\", \n",
    "          \"Feature Analysis & Interpretation\", \"Comprehensive Image Description\"]\n",
    "\n",
    "# 計算各類別的樣本數量\n",
    "sizes = [len(scores) for scores in all_final_average_scores]\n",
    "\n",
    "# 創建圓餅圖 + 統計表格\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# 左側立體圓餅圖\n",
    "labels_short = [\"Image Classification\", \"Microstructural\\nFeature Detection\",\n",
    "               \"Feature Analysis\\n& Interpretation\", \"Comprehensive\\nImage Description\"]\n",
    "\n",
    "# 添加立體效果參數\n",
    "explode = (0.05, 0.05, 0.05, 0.05)  # 稍微分離各扇形\n",
    "wedges, texts, autotexts = ax1.pie(sizes, \n",
    "                                  labels=labels_short, \n",
    "                                  colors=colors, \n",
    "                                  autopct='%1.1f%%', \n",
    "                                  startangle=90, \n",
    "                                  textprops={'fontsize': 14},\n",
    "                                  explode=explode,  # 分離效果\n",
    "                                  wedgeprops=dict(linewidth=2, edgecolor='white'))  # 白色邊框\n",
    "\n",
    "ax1.set_title(\"Dataset Composition by Task Type\", fontsize=16, fontweight='bold')\n",
    "\n",
    "# 右側統計表格 - 修正版本\n",
    "ax2.axis('off')\n",
    "table_data = []\n",
    "total = sum(sizes)\n",
    "\n",
    "for i in range(4):\n",
    "    table_data.append([\n",
    "        labels[i],\n",
    "        f\"{sizes[i]:,}\",\n",
    "        f\"{sizes[i]/total*100:.1f}%\"\n",
    "    ])\n",
    "table_data.append([\"Total\", f\"{total:,}\", \"100.0%\"])\n",
    "\n",
    "table = ax2.table(cellText=table_data, \n",
    "                 colLabels=[\"Task Type\", \"Sample Count\", \"Percentage\"],\n",
    "                 cellLoc='left', loc='center',\n",
    "                 colWidths=[0.5, 0.25, 0.25])  # 調整欄位寬度比例\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12)  # 縮小字體\n",
    "table.scale(1.0, 2.0)  # 增加行高，減少寬度縮放\n",
    "\n",
    "# 美化表格\n",
    "for i in range(len(table_data) + 1):\n",
    "    for j in range(3):\n",
    "        cell = table[(i, j)]\n",
    "        if i == 0:  # 標題行\n",
    "            cell.set_facecolor('#E6E6E6')\n",
    "            cell.set_text_props(weight='bold')\n",
    "        elif i == len(table_data):  # 總計行\n",
    "            cell.set_facecolor('#F0F0F0')\n",
    "            cell.set_text_props(weight='bold')\n",
    "        \n",
    "        # 設定文字換行\n",
    "        if j == 0 and i > 0 and i < len(table_data):  # 任務類型欄位\n",
    "            text = cell.get_text().get_text()\n",
    "            if len(text) > 20:\n",
    "                # 在適當位置換行\n",
    "                if \"Feature\" in text and \"Detection\" in text:\n",
    "                    cell.get_text().set_text(\"Microstructural Feature Detection\")\n",
    "                elif \"Analysis\" in text:\n",
    "                    cell.get_text().set_text(\"Feature Analysis & Interpretation\")\n",
    "                elif \"Comprehensive\" in text:\n",
    "                    cell.get_text().set_text(\"Comprehensive Image Description\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Dataset_composition.png\", dpi=500, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d700920-ea29-4446-bc95-6d21fd1d61cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找出最大的count值來設定統一的Y軸\n",
    "max_count = 0\n",
    "all_counts_for_range = []\n",
    "bin_edges = np.linspace(0, 100, 31)  # 30個bins\n",
    "\n",
    "for i in range(4):\n",
    "    percent = all_final_average_scores[i] * 100\n",
    "    counts, _ = np.histogram(percent, bins=bin_edges)  # 修正：bin_edges 不是 bin*edges\n",
    "    all_counts_for_range.append(counts)\n",
    "    max_count = max(max_count, max(counts))\n",
    "\n",
    "# 設定統一的Y軸範圍 (向上取整到千位)\n",
    "y_max = int(np.ceil(max_count / 1000) * 1000)\n",
    "\n",
    "# 修改2: 調整子圖間距和標題\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 6))  # 稍微加大\n",
    "axes = axes.flatten()\n",
    "\n",
    "# 統計摘要資料\n",
    "stats_summary = []\n",
    "\n",
    "for i in range(4):\n",
    "    percent = all_final_average_scores[i] * 100\n",
    "    \n",
    "    # 繪製直方圖\n",
    "    n, bins, patches = axes[i].hist(percent, bins=30, color=colors[i], \n",
    "                                   alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "    # 計算統計值\n",
    "    mean_val = np.mean(percent)\n",
    "    std_val = np.std(percent)\n",
    "    \n",
    "    # 簡化標題\n",
    "    axes[i].set_title(f'{labels[i]}\\nμ={mean_val:.1f}, σ={std_val:.1f}', \n",
    "                     fontsize=16, fontweight='bold', pad=10)\n",
    "    \n",
    "    # 簡化圖例\n",
    "    axes[i].axvline(mean_val, color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "    axes[i].legend(fontsize=8, loc='upper right')\n",
    "    \n",
    "    # 添加X軸和Y軸標籤（英文）\n",
    "    axes[i].set_xlabel('Score', fontsize=14, fontweight='bold')\n",
    "    axes[i].set_ylabel('Count', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 統一軸範圍 - 這是關鍵！\n",
    "    axes[i].set_xlim(0, 100)\n",
    "    axes[i].set_ylim(0, 12000)  # 統一Y軸範圍讓數量差異更明顯\n",
    "    \n",
    "    # 美化刻度標籤\n",
    "    axes[i].tick_params(axis='x', labelsize=12)\n",
    "    axes[i].tick_params(axis='y', labelsize=12)\n",
    "    \n",
    "    # 添加網格\n",
    "    axes[i].grid(True, alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # 收集統計資料\n",
    "    stats_summary.append({\n",
    "        'Task Type': labels[i],\n",
    "        'Sample Size': len(percent),\n",
    "        'Mean': mean_val,\n",
    "        'Std Dev': std_val,\n",
    "        'Max Count': max(n)\n",
    "    })\n",
    "\n",
    "# 修改3: 調整整體佈局\n",
    "fig.suptitle('Task Difficulty Distribution Analysis\\n(Unified Y-axis for quantity comparison)', \n",
    "             fontsize=20, fontweight='bold', y=0.95)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.65, hspace=0.3, wspace=0.3)  # 調整間距\n",
    "plt.savefig(\"Difficulty_Distributions_with.png\", dpi=500, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520be9dd-03e1-4077-b5e2-60bf593d2fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合併所有資料並分級\n",
    "all_scores = np.concatenate([scores * 100 for scores in all_final_average_scores])\n",
    "all_task_types = []\n",
    "\n",
    "for i, scores in enumerate(all_final_average_scores):\n",
    "    all_task_types.extend([labels[i]] * len(scores))\n",
    "\n",
    "# 使用三等份方式分級（根據分數排序）\n",
    "sorted_scores = np.sort(all_scores)\n",
    "total_samples = len(sorted_scores)\n",
    "\n",
    "# 計算三等份的分界點\n",
    "hard_threshold = sorted_scores[total_samples // 3]\n",
    "medium_threshold = sorted_scores[2 * total_samples // 3]\n",
    "\n",
    "def classify_difficulty_tertile(score):\n",
    "    if score <= hard_threshold:\n",
    "        return \"Hard\"\n",
    "    elif score <= medium_threshold:\n",
    "        return \"Medium\"  \n",
    "    else:\n",
    "        return \"Easy\"\n",
    "\n",
    "# 應用分級\n",
    "difficulties = [classify_difficulty_tertile(score) for score in all_scores]\n",
    "\n",
    "# 創建DataFrame\n",
    "df_curriculum = pd.DataFrame({\n",
    "    'score': all_scores,\n",
    "    'task_type': all_task_types,\n",
    "    'difficulty': difficulties\n",
    "})\n",
    "\n",
    "# 計算各難度等級的任務分布\n",
    "task_difficulty_counts = df_curriculum.groupby(['difficulty', 'task_type']).size().unstack(fill_value=0)\n",
    "\n",
    "# 創建圖表：垂直堆疊柱狀圖 + 統計表格\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "\n",
    "# 精確定義每個子圖的位置和大小 [left, bottom, width, height]\n",
    "ax1 = fig.add_axes([0.1, 0.2, 0.4, 0.6])   # 左圖：從10%開始，寬60%\n",
    "ax2 = fig.add_axes([0.6, 0.1, 0.2, 0.6])  # 右圖：從75%開始，寬20%\n",
    "\n",
    "difficulty_order = ['Easy', 'Medium', 'Hard']\n",
    "\n",
    "# 左圖：垂直堆疊柱狀圖\n",
    "bottom = np.zeros(len(difficulty_order))\n",
    "bar_width = 0.6  # 柱狀圖寬度\n",
    "\n",
    "for i, task_type in enumerate(labels):\n",
    "    if task_type in task_difficulty_counts.columns:\n",
    "        values = [task_difficulty_counts.loc[diff, task_type] if diff in task_difficulty_counts.index else 0 \n",
    "                 for diff in difficulty_order]\n",
    "        \n",
    "        bars = ax1.bar(difficulty_order, values, bottom=bottom, width=bar_width,\n",
    "                      color=colors[i], alpha=0.8, label=task_type, \n",
    "                      edgecolor='white', linewidth=1)\n",
    "        \n",
    "        # 在每個區段標註百分比和數量\n",
    "        for j, (bar, value) in enumerate(zip(bars, values)):\n",
    "            if value > 0:\n",
    "                total_for_difficulty = sum([task_difficulty_counts.loc[difficulty_order[j], task] \n",
    "                                          if task in task_difficulty_counts.columns and difficulty_order[j] in task_difficulty_counts.index else 0 \n",
    "                                          for task in labels])\n",
    "                percentage = (value / total_for_difficulty) * 100 if total_for_difficulty > 0 else 0\n",
    "                \n",
    "                # 只在足夠大的區段顯示文字\n",
    "                if percentage > 1:\n",
    "                    y_pos = bottom[j] + value / 2\n",
    "                    ax1.text(j, y_pos, f'{percentage:.1f}%', \n",
    "                            ha='center', va='center', fontsize=10, fontweight='bold',\n",
    "                            color='white',\n",
    "                            bbox=dict(boxstyle='round,pad=0.2', facecolor='black', alpha=0.7))\n",
    "        \n",
    "        bottom += values\n",
    "\n",
    "# 美化垂直柱狀圖\n",
    "ax1.set_xlabel('Difficulty Level', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Sample Count', fontsize=12, fontweight='bold')\n",
    "ax1.legend(title='Task Types', fontsize=10, title_fontsize=11, \n",
    "          bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 添加總數標註\n",
    "for i, diff in enumerate(difficulty_order):\n",
    "    if diff in task_difficulty_counts.index:\n",
    "        total_for_diff = sum([task_difficulty_counts.loc[diff, task] \n",
    "                             if task in task_difficulty_counts.columns else 0 \n",
    "                             for task in labels])\n",
    "        ax1.text(i, total_for_diff + max(bottom) * 0.02, f'Total: {total_for_diff:,}', \n",
    "                ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# 右圖：詳細統計表格\n",
    "ax2.axis('off')\n",
    "\n",
    "# 準備表格資料\n",
    "table_data = []\n",
    "headers = ['Difficulty\\nLevel'] + ['Image\\nClassification', 'Feature\\nDetection', 'Feature\\nAnalysis', 'Image\\nDescription'] + ['Total']\n",
    "\n",
    "for diff in difficulty_order:\n",
    "    row_data = [diff]\n",
    "    total_for_diff = 0\n",
    "    for task_type in labels:\n",
    "        if task_type in task_difficulty_counts.columns and diff in task_difficulty_counts.index:\n",
    "            count = task_difficulty_counts.loc[diff, task_type]\n",
    "        else:\n",
    "            count = 0\n",
    "        row_data.append(f\"{count:,}\")\n",
    "        total_for_diff += count\n",
    "    row_data.append(f\"{total_for_diff:,}\")\n",
    "    table_data.append(row_data)\n",
    "\n",
    "# 添加總計行\n",
    "total_row = ['Total']\n",
    "for task_type in labels:\n",
    "    if task_type in task_difficulty_counts.columns:\n",
    "        total_for_task = sum(task_difficulty_counts[task_type])\n",
    "    else:\n",
    "        total_for_task = 0\n",
    "    total_row.append(f\"{total_for_task:,}\")\n",
    "total_row.append(f\"{len(all_scores):,}\")\n",
    "table_data.append(total_row)\n",
    "\n",
    "# 創建表格\n",
    "table = ax2.table(cellText=table_data, colLabels=headers,\n",
    "                 cellLoc='center', loc='center',\n",
    "                 colWidths=[0.2, 0.2, 0.2, 0.2, 0.2, 0.2])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.5, 3)\n",
    "\n",
    "# 美化表格\n",
    "for i in range(len(table_data) + 1):\n",
    "    for j in range(len(headers)):\n",
    "        cell = table[(i, j)]\n",
    "        if i == 0:  # 標題行\n",
    "            if j == 0:  # 難度等級欄位\n",
    "                cell.set_facecolor('#E6E6E6')\n",
    "            elif 1 <= j <= 4:  # 任務類別欄位\n",
    "                cell.set_facecolor(colors[j-1])\n",
    "            else:  # 總計欄位\n",
    "                cell.set_facecolor('#E6E6E6')\n",
    "            cell.set_text_props(weight='bold', fontsize=10)\n",
    "        elif i == len(table_data):  # 總計行\n",
    "            cell.set_facecolor('#F0F0F0')\n",
    "            cell.set_text_props(weight='bold', fontsize=10)\n",
    "\n",
    "# 整體標題\n",
    "fig.suptitle('Curriculum Learning Analysis: Task Type Distribution Across Difficulty Levels', \n",
    "             fontsize=16, fontweight='bold', y=0.9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.85)\n",
    "plt.savefig(\"curriculum_dataset.png\", dpi=500, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a3af1b-eeb9-47a3-97d9-a27a32035c0e",
   "metadata": {},
   "source": [
    "# For 3 level training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae00d3b2-8316-4cbe-a5ff-50c63514de74",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 讀取與合併\n",
    "df_list = [pd.read_csv(file.replace(\".csv\", \"_final_scored.csv\")) for file in answers]\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# 確保必要欄位存在\n",
    "required_columns = {\"image\", \"question\", \"expected\", \"final_score_average\"}\n",
    "assert required_columns.issubset(df.columns), f\"缺少欄位：{required_columns - set(df.columns)}\"\n",
    "\n",
    "# 計算分位數並分級\n",
    "df[\"score\"] = df[\"final_score_average\"] * 100\n",
    "q1 = df[\"score\"].quantile(1/3)\n",
    "q2 = df[\"score\"].quantile(2/3)\n",
    "\n",
    "def assign_difficulty(score):\n",
    "    if score <= q1:\n",
    "        return \"Hard\"\n",
    "    elif score <= q2:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Easy\"\n",
    "\n",
    "df[\"difficulty\"] = df[\"score\"].apply(assign_difficulty)\n",
    "\n",
    "# LLaVA 格式生成函數\n",
    "def make_convo(row):\n",
    "    return {\n",
    "        \"id\": f\"{row['difficulty'].upper()}_{row.name:06d}\",\n",
    "        \"image\": f\"{row['image']}\",\n",
    "        \"conversations\": [\n",
    "            {\"from\": \"human\", \"value\": f\"<image>\\n{row['question']}\"},\n",
    "            {\"from\": \"gpt\", \"value\": row[\"expected\"]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# 依照難度輸出為 JSONL 檔案\n",
    "for level in [\"Hard\", \"Medium\", \"Easy\"]:\n",
    "    subset = df[df[\"difficulty\"] == level]\n",
    "    items = [make_convo(row) for _, row in subset.iterrows()]\n",
    "    \n",
    "    with open(f\"llava_train_{level.lower()}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(items, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ 已輸出為 LLaVA 格式：llava_train_hard.jsonl / medium / easy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
