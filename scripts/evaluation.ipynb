{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21a1cca0-089c-4900-9619-405009a62227",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c5e380-93c8-4581-abd9-0f4f10882739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import gc\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine learning and NLP\n",
    "import evaluate\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoModel, AutoTokenizer, pipeline\n",
    "\n",
    "# API clients\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a79be05-6ccf-4692-b5f1-cc70717e627b",
   "metadata": {},
   "source": [
    "# The main scoring section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a20e06a-9b15-4147-ae57-44ab85e19a01",
   "metadata": {},
   "source": [
    "## Set up for GPT as judge to do the human-like scoring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c99b10-bfbb-4901-9479-df352a1a4f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/NE6131039/Desktop/Confidential_Key.txt\", \"r\") as f:\n",
    "    api_key = f.read().strip()\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "GPT_MODEL = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafa4f82-a38b-4705-a3c7-4cb9d266b7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gpt_score_prompt(prediction, reference, question):\n",
    "# Prompt construction logic omitted for proprietary reasons\n",
    "# Contact authors for detailed evaluation methodology\n",
    "    return f\"\"\"\n",
    "    Builds evaluation prompt for GPT to score TEM image analysis responses.\n",
    "    \n",
    "    The prompt instructs GPT to:\n",
    "    - Act as a TEM domain expert\n",
    "    - Evaluate prediction against reference answer\n",
    "    - Consider scientific accuracy, completeness, and technical precision\n",
    "    - Return numerical score (0.00-1.00) only\n",
    "    \n",
    "    Args:\n",
    "        prediction (str): Model's predicted answer\n",
    "        reference (str): Ground truth reference answer  \n",
    "        question (str): Original question asked\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted evaluation prompt\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def gpt_score(prediction, reference, question):\n",
    "    prompt = build_gpt_score_prompt(prediction, reference, question)\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=GPT_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0,\n",
    "        )\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        \n",
    "        match = re.search(r\"(\\d+(?:\\.\\d{1,2})?)\", content)\n",
    "        if match:\n",
    "            score = float(match.group(1))\n",
    "            score = min(1.0, max(0.0, score))\n",
    "            return round(score, 2)\n",
    "        \n",
    "\n",
    "        print(f\"[GPT PARSE WARNING] Could not parse score from: {content}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[GPT ERROR] {e}\")\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d2cda8-c973-4c36-a4ce-a0e88d5e34f7",
   "metadata": {},
   "source": [
    "## Load in files & scoring metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7a857e-78a7-4b2e-a715-94df5749ac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = [\n",
    "    \"pretrain_val_predict.csv\",\n",
    "    \"finetune_no_curriculum_val_predict.csv\",\n",
    "    # \"finetune_curriculum_val_predict.csv\",\n",
    "    \"finetune_curriculum_final_val_predict.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90950608-c607-4da3-bb0c-67254a312a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = evaluate.load(\"bleu\")\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "sbert = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "device = torch.device(\"cuda:2\")\n",
    "sbert = sbert.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44228b19-8f66-4309-a081-2d8831dc6702",
   "metadata": {},
   "source": [
    "## all data scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e552b3-26a3-4127-bad3-a63deead3957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Main Loop ====\n",
    "for answer in answers:\n",
    "    df = pd.read_csv(answer)\n",
    "    \n",
    "    sample_df = df.groupby('type').sample(n=2500, random_state=42)\n",
    "    df = sample_df.reset_index(drop=True)\n",
    "    \n",
    "    references = df[\"expected\"].astype(str).tolist()\n",
    "    predictions = df[\"predicted\"].astype(str).tolist()\n",
    "    questions = df[\"question\"].astype(str).tolist()\n",
    "\n",
    "    # =======================\n",
    "    #        GPT_SCORE\n",
    "    # =======================\n",
    "    gpt_scores = []\n",
    "    for pred, ref,que in tqdm(zip(predictions, references,questions), total=len(predictions), desc=\"GPT Scoring\"):\n",
    "        gpt_scores.append(gpt_score(pred, ref,que))\n",
    "\n",
    "    df[\"gpt_score\"] = gpt_scores\n",
    "    \n",
    "    lexical_scores = []\n",
    "    bleu_combined_scores = []\n",
    "    rouge_combined_scores = []\n",
    "    meteor_scores = []\n",
    "    # =======================\n",
    "    #        LEXICAL\n",
    "    # =======================\n",
    "    for pred, ref in tqdm(zip(predictions, references), total=len(predictions)):\n",
    "        try:\n",
    "            \n",
    "            # BLEU-1 to BLEU-4 (get all max_order BLEU scores)\n",
    "            bleu_scores_all = bleu.compute(predictions=[pred], references=[[ref]], max_order=4, smooth=True)\n",
    "            bleu1 = bleu_scores_all.get(\"precisions\", [0, 0, 0, 0])[0]\n",
    "            bleu2 = bleu_scores_all.get(\"precisions\", [0, 0, 0, 0])[1]\n",
    "            bleu3 = bleu_scores_all.get(\"precisions\", [0, 0, 0, 0])[2]\n",
    "            bleu4 = bleu_scores_all.get(\"precisions\", [0, 0, 0, 0])[3]\n",
    "            \n",
    "            bleu_combined  = 0.4*bleu1+0.3*bleu2+0.2*bleu3+0.1*bleu4\n",
    "            \n",
    "            #METEOR \n",
    "            meteor_score = meteor.compute(predictions=[pred], references=[ref])[\"meteor\"]\n",
    "\n",
    "            #ROUGE\n",
    "            rouge_score = rouge.compute(predictions=[pred], references=[ref])\n",
    "            rouge_1 = rouge_score.get(\"rouge1\", 0.0)\n",
    "            rouge_2 = rouge_score.get(\"rouge2\", 0.0)\n",
    "            rouge_l = rouge_score.get(\"rougeL\", 0.0)\n",
    "            rouge_lsum = rouge_score.get(\"rougeLsum\", 0.0)\n",
    "            \n",
    "            rouge_combined = (rouge_1 + rouge_2 + ((rouge_l + rouge_lsum) / 2)) / 3\n",
    "\n",
    "            score = (\n",
    "                0.3 * rouge_combined +\n",
    "                0.2 * bleu_combined +\n",
    "                0.5 * meteor_score\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error on sample: {e}\")\n",
    "            score = 0.0\n",
    "            \n",
    "        # Append metrics\n",
    "        bleu_combined_scores.append(round(min(max(bleu_combined, 0.0), 1.0), 4))\n",
    "        rouge_combined_scores.append(round(min(max(rouge_combined, 0.0), 1.0), 4))\n",
    "        meteor_scores.append(round(min(max(meteor_score, 0.0), 1.0), 4))\n",
    "        lexical_scores.append(round(min(max(score, 0.0), 1.0), 4))\n",
    "        \n",
    "    df[\"bleu_scores\"] = bleu_combined_scores\n",
    "    df[\"rouge_scores\"] = rouge_combined_scores\n",
    "    df[\"meteor_scores\"] = meteor_scores\n",
    "    df[\"lexical_scores\"] = lexical_scores\n",
    "    \n",
    "\n",
    "    # =======================\n",
    "    #        SEMANTIC\n",
    "    # =======================\n",
    "    bert_scores = []\n",
    "    sbert_scores = []\n",
    "    semantic_scores = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing {answer}\"):\n",
    "        pred = str(row[\"predicted\"])\n",
    "        ref = str(row[\"expected\"])\n",
    "\n",
    "        # --- BERTScore ---\n",
    "        try:\n",
    "            bert_result = bertscore.compute(\n",
    "                predictions=[pred],\n",
    "                references=[ref],\n",
    "                lang=\"en\",\n",
    "                model_type=\"microsoft/deberta-xlarge-mnli\",\n",
    "                device=\"cuda:3\"\n",
    "            )\n",
    "            bert_f1 = bert_result[\"f1\"][0]\n",
    "        except Exception as e:\n",
    "            print(f\"BERTScore error: {e}\")\n",
    "            bert_f1 = 0.0\n",
    "\n",
    "        # --- SBERT cosine similarity ---\n",
    "        try:\n",
    "            emb_pred = sbert.encode(pred, convert_to_tensor=True, device=\"cuda:2\")\n",
    "            emb_ref = sbert.encode(ref, convert_to_tensor=True, device=\"cuda:2\")\n",
    "            sim = util.cos_sim(emb_pred, emb_ref).item()\n",
    "        except Exception as e:\n",
    "            print(f\"SBERT error: {e}\")\n",
    "            sim = 0.0\n",
    "\n",
    "        # Average semantic score\n",
    "        avg = (bert_f1 + sim) / 2\n",
    "\n",
    "        # Accumulate\n",
    "        bert_scores.append(round(min(max(bert_f1, 0.0), 1.0), 4))\n",
    "        sbert_scores.append(round(min(max(sim, 0.0), 1.0), 4))\n",
    "        semantic_scores.append(round(min(max(avg, 0.0), 1.0), 4))\n",
    "\n",
    "        # Optional: cleanup per-row\n",
    "        del emb_pred, emb_ref\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # Save\n",
    "    df[\"bert_scores\"] = bert_scores\n",
    "    df[\"sbert_scores\"] = sbert_scores\n",
    "    df[\"semantic_scores\"] = semantic_scores\n",
    "   \n",
    "    # # =======================\n",
    "    # #     FINAL SCORE\n",
    "    # # =======================\n",
    "    lexical_np = np.array(lexical_scores)\n",
    "    semantic_np = np.array(semantic_scores)\n",
    "\n",
    "    #average\n",
    "    final_scores = 0.5 * lexical_np +0.5* semantic_np\n",
    "    df[\"final_scores\"] = final_scores\n",
    "    \n",
    "    # df.drop(columns=[\"image\", \"question\", \"expected\", \"predicted\"], inplace=True)\n",
    "    df.to_csv(answer.replace(\".csv\", \"_scored.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
